# Word Parser Implementation Guide

## 🎯 Goal
Parse `.docx` files generated by our system (or hand-edited) back into structured data for Excel generation.

## 📦 Required Package
```bash
npm install docx mammoth
```

**Why two packages?**
- `docx` - We already use this for generation, has limited parsing
- `mammoth` - Better for reading/extracting content from existing .docx files

## 🏗️ File: `/lib/wordParser.js`

### Core Function Signature
```javascript
/**
 * Parse a Word document into structured data
 * @param {File} file - The .docx file from FileDropZone
 * @returns {Promise<ParsedDocument>}
 */
export async function parseWordDocument(file) {
  // Implementation
}

/**
 * ParsedDocument structure:
 * {
 *   market: string,              // Detected from filename (e.g., "en-US")
 *   sections: [
 *     {
 *       deliverable: string,     // Section name (e.g., "Gallery 1")
 *       fields: [
 *         {
 *           name: string,        // Field name (e.g., "Headline")
 *           content: string,     // The actual copy
 *           confidence: number   // 0-1, for fuzzy matching
 *         }
 *       ]
 *     }
 *   ],
 *   metadata: {
 *     filename: string,
 *     parsedAt: Date,
 *     warnings: string[]
 *   }
 * }
 */
```

## 🔍 Parsing Strategy

### Step 1: Extract Raw Text with Structure
```javascript
import mammoth from 'mammoth';

async function extractStructure(file) {
  const arrayBuffer = await file.arrayBuffer();
  
  const result = await mammoth.convertToHtml(arrayBuffer, {
    styleMap: [
      "p[style-name='Heading 1'] => h1:fresh",
      "p[style-name='Heading 2'] => h2:fresh",
    ]
  });
  
  // result.value contains HTML with headings preserved
  // result.messages contains warnings (if any)
  
  return {
    html: result.value,
    warnings: result.messages
  };
}
```

### Step 2: Parse HTML Structure
```javascript
function parseHTML(html) {
  // Use DOMParser or cheerio-like parsing
  const parser = new DOMParser();
  const doc = parser.parseFromString(html, 'text/html');
  
  const sections = [];
  let currentSection = null;
  
  // Walk through all elements
  doc.body.childNodes.forEach(node => {
    if (node.tagName === 'H1') {
      // New section
      if (currentSection) sections.push(currentSection);
      currentSection = {
        deliverable: node.textContent.trim(),
        fields: []
      };
    }
    else if (node.tagName === 'P' && currentSection) {
      // Look for "FieldName: Content" pattern
      const text = node.textContent;
      const match = text.match(/^(.+?):\s*(.*)$/);
      
      if (match) {
        currentSection.fields.push({
          name: match[1].trim(),
          content: match[2].trim(),
          confidence: 1.0
        });
      }
    }
  });
  
  if (currentSection) sections.push(currentSection);
  
  return sections;
}
```

### Step 3: Detect Market from Filename
```javascript
function detectMarket(filename) {
  // Pattern: Marketing_Copy_en-US.docx → "en-US"
  const marketMatch = filename.match(/_([a-z]{2}-[A-Z]{2})\.docx$/i);
  
  if (marketMatch) {
    return marketMatch[1];
  }
  
  // Fallback: try to find any locale code pattern
  const fallbackMatch = filename.match(/([a-z]{2}-[A-Z]{2})/i);
  return fallbackMatch ? fallbackMatch[1] : 'unknown';
}
```

### Step 4: Fuzzy Field Name Matching
```javascript
import deliverables from '@/data/deliverables.json';

function fuzzyMatchField(fieldName, expectedFields) {
  // Exact match first
  if (expectedFields.includes(fieldName)) {
    return { match: fieldName, confidence: 1.0 };
  }
  
  // Normalize and try again
  const normalized = fieldName.toLowerCase().trim();
  const normalizedExpected = expectedFields.map(f => f.toLowerCase().trim());
  
  const exactIndex = normalizedExpected.indexOf(normalized);
  if (exactIndex !== -1) {
    return { match: expectedFields[exactIndex], confidence: 0.95 };
  }
  
  // Common variations
  const variations = {
    'headline': ['title', 'header', 'heading'],
    'body': ['description', 'copy', 'text', 'content'],
    'cta': ['button', 'call to action', 'action'],
    'legal': ['disclaimer', 'fine print', 'terms']
  };
  
  for (const [standard, alts] of Object.entries(variations)) {
    if (alts.includes(normalized) && expectedFields.includes(standard)) {
      return { match: standard, confidence: 0.8 };
    }
  }
  
  // No match found
  return { match: fieldName, confidence: 0.0 };
}

function validateFieldNames(sections) {
  // Get all expected deliverables/fields from catalog
  const catalog = Object.values(deliverables);
  
  sections.forEach(section => {
    // Find matching deliverable in catalog
    const deliverable = catalog.find(d => 
      d.sections?.some(s => s.name === section.deliverable)
    );
    
    if (deliverable) {
      const expectedSection = deliverable.sections.find(s => 
        s.name === section.deliverable
      );
      
      if (expectedSection) {
        // Validate each field
        section.fields = section.fields.map(field => {
          const { match, confidence } = fuzzyMatchField(
            field.name, 
            expectedSection.fields
          );
          
          return {
            ...field,
            originalName: field.name,
            name: match,
            confidence
          };
        });
      }
    }
  });
  
  return sections;
}
```

## 🔧 Complete Implementation

```javascript
// lib/wordParser.js
import mammoth from 'mammoth';
import deliverables from '@/data/deliverables.json';

export async function parseWordDocument(file) {
  try {
    // Step 1: Extract structure
    const arrayBuffer = await file.arrayBuffer();
    const result = await mammoth.convertToHtml(arrayBuffer);
    
    // Step 2: Parse HTML
    const sections = parseHTML(result.value);
    
    // Step 3: Detect market
    const market = detectMarket(file.name);
    
    // Step 4: Validate field names
    const validatedSections = validateFieldNames(sections);
    
    // Step 5: Compile warnings
    const warnings = [];
    
    validatedSections.forEach(section => {
      section.fields.forEach(field => {
        if (field.confidence < 0.8) {
          warnings.push(
            `Low confidence match: "${field.originalName}" → "${field.name}" (${Math.round(field.confidence * 100)}%)`
          );
        }
      });
    });
    
    return {
      market,
      sections: validatedSections,
      metadata: {
        filename: file.name,
        parsedAt: new Date(),
        warnings
      }
    };
    
  } catch (error) {
    console.error('Error parsing Word document:', error);
    throw new Error(`Failed to parse ${file.name}: ${error.message}`);
  }
}

// Helper: Parse HTML structure
function parseHTML(html) {
  const parser = new DOMParser();
  const doc = parser.parseFromString(html, 'text/html');
  
  const sections = [];
  let currentSection = null;
  
  const walker = document.createTreeWalker(
    doc.body,
    NodeFilter.SHOW_ELEMENT,
    null
  );
  
  let node;
  while (node = walker.nextNode()) {
    if (node.tagName === 'H1') {
      if (currentSection) sections.push(currentSection);
      currentSection = {
        deliverable: node.textContent.trim(),
        fields: []
      };
    }
    else if (node.tagName === 'P' && currentSection) {
      const text = node.textContent;
      const match = text.match(/^(.+?):\s*(.*)$/);
      
      if (match && match[1] && match[2]) {
        currentSection.fields.push({
          name: match[1].trim(),
          content: match[2].trim(),
          confidence: 1.0
        });
      }
    }
  }
  
  if (currentSection) sections.push(currentSection);
  
  return sections;
}

// Helper: Detect market from filename
function detectMarket(filename) {
  const marketMatch = filename.match(/_([a-z]{2}-[A-Z]{2})\.docx$/i);
  return marketMatch ? marketMatch[1] : 'unknown';
}

// Helper: Fuzzy field matching
function fuzzyMatchField(fieldName, expectedFields) {
  if (expectedFields.includes(fieldName)) {
    return { match: fieldName, confidence: 1.0 };
  }
  
  const normalized = fieldName.toLowerCase().trim();
  const normalizedExpected = expectedFields.map(f => f.toLowerCase().trim());
  
  const exactIndex = normalizedExpected.indexOf(normalized);
  if (exactIndex !== -1) {
    return { match: expectedFields[exactIndex], confidence: 0.95 };
  }
  
  const variations = {
    'headline': ['title', 'header', 'heading'],
    'body': ['description', 'copy', 'text', 'content'],
    'cta': ['button', 'call to action', 'action'],
    'legal': ['disclaimer', 'fine print', 'terms']
  };
  
  for (const [standard, alts] of Object.entries(variations)) {
    if (alts.includes(normalized) && expectedFields.includes(standard)) {
      return { match: standard, confidence: 0.8 };
    }
  }
  
  return { match: fieldName, confidence: 0.0 };
}

// Helper: Validate field names
function validateFieldNames(sections) {
  const catalog = Object.values(deliverables);
  
  sections.forEach(section => {
    const deliverable = catalog.find(d => 
      d.sections?.some(s => s.name === section.deliverable)
    );
    
    if (deliverable) {
      const expectedSection = deliverable.sections.find(s => 
        s.name === section.deliverable
      );
      
      if (expectedSection) {
        section.fields = section.fields.map(field => {
          const { match, confidence } = fuzzyMatchField(
            field.name, 
            expectedSection.fields
          );
          
          return {
            ...field,
            originalName: field.name !== match ? field.name : undefined,
            name: match,
            confidence
          };
        });
      }
    }
  });
  
  return sections;
}

// Batch parse multiple Word documents
export async function parseMultipleWordDocuments(files) {
  const results = await Promise.allSettled(
    files.map(file => parseWordDocument(file))
  );
  
  const parsed = [];
  const errors = [];
  
  results.forEach((result, index) => {
    if (result.status === 'fulfilled') {
      parsed.push(result.value);
    } else {
      errors.push({
        filename: files[index].name,
        error: result.reason.message
      });
    }
  });
  
  return { parsed, errors };
}
```

## ✅ Testing Checklist

- [ ] Parse Word doc generated by Phase 1 (perfect match)
- [ ] Parse manually edited Word doc (fuzzy matching)
- [ ] Handle missing sections gracefully
- [ ] Handle unexpected field names
- [ ] Detect market code correctly
- [ ] Multiple docs at once (batch parsing)
- [ ] Error handling for corrupted files
- [ ] Warning messages for low confidence matches

## 🚨 Edge Cases to Handle

1. **Missing Sections** - User deleted a section
2. **Extra Sections** - User added custom sections
3. **Renamed Fields** - User changed "Headline" to "Title"
4. **Empty Fields** - Field exists but no content
5. **Formatting** - User added bold/italic (strip it)
6. **Multiple Colons** - "Field: Content: More" (take everything after first colon)
7. **No Colons** - Plain text paragraph (skip or flag?)

## 📊 Expected Performance

- **Single doc (50 fields):** <500ms
- **10 docs batch:** <3 seconds
- **Memory:** ~10MB per document

## 🔗 Next Steps

After implementing this, read:
- `excel-parser-guide.md` for reverse direction
- `component-specs.md` for UI integration
